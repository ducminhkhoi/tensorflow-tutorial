{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Average loss epoch 0: 1.28798831426\n",
      "Average loss epoch 1: 0.731789389274\n",
      "Average loss epoch 2: 0.600046665379\n",
      "Average loss epoch 3: 0.536560050396\n",
      "Average loss epoch 4: 0.497979920783\n",
      "Average loss epoch 5: 0.471152321789\n",
      "Average loss epoch 6: 0.45136976381\n",
      "Average loss epoch 7: 0.436003095551\n",
      "Average loss epoch 8: 0.423502501977\n",
      "Average loss epoch 9: 0.413297791025\n",
      "Average loss epoch 10: 0.404458430264\n",
      "Average loss epoch 11: 0.397025398545\n",
      "Average loss epoch 12: 0.390359385239\n",
      "Average loss epoch 13: 0.384593329056\n",
      "Average loss epoch 14: 0.379625866324\n",
      "Average loss epoch 15: 0.374751108996\n",
      "Average loss epoch 16: 0.37043669007\n",
      "Average loss epoch 17: 0.366647427593\n",
      "Average loss epoch 18: 0.363134206244\n",
      "Average loss epoch 19: 0.359895274877\n",
      "Average loss epoch 20: 0.356851126591\n",
      "Average loss epoch 21: 0.35413540997\n",
      "Average loss epoch 22: 0.351266683051\n",
      "Average loss epoch 23: 0.34896270375\n",
      "Average loss epoch 24: 0.346618399649\n",
      "Average loss epoch 25: 0.344489328685\n",
      "Average loss epoch 26: 0.342281322865\n",
      "Average loss epoch 27: 0.340465443013\n",
      "Average loss epoch 28: 0.338603112525\n",
      "Average loss epoch 29: 0.337046360185\n",
      "Total time: 31.0279350281 seconds\n",
      "Optimization Finished!\n",
      "Accuracy 0.9118\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Simple logistic regression model to solve OCR task \n",
    "with MNIST in TensorFlow\n",
    "MNIST dataset: yann.lecun.com/exdb/mnist/\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import time\n",
    "# Define paramaters for the model\n",
    "learning_rate = 0.01\n",
    "batch_size = 128\n",
    "n_epochs = 30\n",
    "\n",
    "# Step 1: Read in data\n",
    "# using TF Learn's built in function to load MNIST data to the folder data/mnist\n",
    "mnist = input_data.read_data_sets('data/mnist', one_hot=True) \n",
    "\n",
    "# Step 2: create placeholders for features and labels\n",
    "# each image in the MNIST data is of shape 28*28 = 784\n",
    "# therefore, each image is represented with a 1x784 tensor\n",
    "# there are 10 classes for each image, corresponding to digits 0 - 9. \n",
    "# each lable is one hot vector.\n",
    "X = tf.placeholder(tf.float32, [batch_size, 784], name='X_placeholder') \n",
    "Y = tf.placeholder(tf.float32, [batch_size, 10], name='Y_placeholder')\n",
    "\n",
    "# Step 3: create weights and bias\n",
    "# w is initialized to random variables with mean of 0, stddev of 0.01\n",
    "# b is initialized to 0\n",
    "# shape of w depends on the dimension of X and Y so that Y = tf.matmul(X, w)\n",
    "# shape of b depends on Y\n",
    "w = tf.Variable(tf.random_normal(shape=[784, 10], stddev=0.01), name='weights')\n",
    "b = tf.Variable(tf.zeros([1, 10]), name=\"bias\")\n",
    "\n",
    "# Step 4: build model\n",
    "# the model that returns the logits.\n",
    "# this logits will be later passed through softmax layer\n",
    "logits = tf.matmul(X, w) + b \n",
    "\n",
    "# Step 5: define loss function\n",
    "# use cross entropy of softmax of logits as the loss function\n",
    "entropy = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y, name='loss')\n",
    "loss = tf.reduce_mean(entropy) # computes the mean over all the examples in the batch\n",
    "\n",
    "# Step 6: define training op\n",
    "# using gradient descent with learning rate of 0.01 to minimize loss\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\t# to visualize using TensorBoard\n",
    "\twriter = tf.summary.FileWriter('./my_graph/03/logistic_reg', sess.graph)\n",
    "\n",
    "\tstart_time = time.time()\n",
    "\tsess.run(tf.global_variables_initializer())\t\n",
    "\tn_batches = int(mnist.train.num_examples/batch_size)\n",
    "\tfor i in range(n_epochs): # train the model n_epochs times\n",
    "\t\ttotal_loss = 0\n",
    "\n",
    "\t\tfor _ in range(n_batches):\n",
    "\t\t\tX_batch, Y_batch = mnist.train.next_batch(batch_size)\n",
    "\t\t\t_, loss_batch = sess.run([optimizer, loss], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\t\ttotal_loss += loss_batch\n",
    "\t\tprint 'Average loss epoch {0}: {1}'.format(i, total_loss/n_batches)\n",
    "\n",
    "\tprint 'Total time: {0} seconds'.format(time.time() - start_time)\n",
    "\n",
    "\tprint('Optimization Finished!') # should be around 0.35 after 25 epochs\n",
    "\n",
    "\t# test the model\n",
    "\tn_batches = int(mnist.test.num_examples/batch_size)\n",
    "\ttotal_correct_preds = 0\n",
    "\tfor i in range(n_batches):\n",
    "\t\tX_batch, Y_batch = mnist.test.next_batch(batch_size)\n",
    "\t\t_, loss_batch, logits_batch = sess.run([optimizer, loss, logits], feed_dict={X: X_batch, Y:Y_batch}) \n",
    "\t\tpreds = tf.nn.softmax(logits_batch)\n",
    "\t\tcorrect_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(Y_batch, 1))\n",
    "\t\taccuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32)) # need numpy.count_nonzero(boolarr) :(\n",
    "\t\ttotal_correct_preds += sess.run(accuracy)\t\n",
    "\t\n",
    "\tprint 'Accuracy {0}'.format(total_correct_preds/mnist.test.num_examples)\n",
    "\n",
    "\twriter.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((60000, 28, 28), array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 1.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       ..., \n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
      "       [ 0.,  0.,  0., ...,  0.,  1.,  0.]]))\n",
      "Epoch 1/30"
     ]
    }
   ],
   "source": [
    "# Keras equivalence\n",
    "%matplotlib inline\n",
    "from keras.datasets import mnist\n",
    "import keras\n",
    "from keras.models import Model, Input, Sequential\n",
    "from keras.layers.core import Dense, Flatten, Reshape\n",
    "from keras.optimizers import SGD\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "print(x_train.shape, y_train.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(28, 28)))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(SGD(learning_rate), loss='categorical_crossentropy')\n",
    "\n",
    "model.fit(x_train, y_train, epochs=n_epochs, batch_size=batch_size)\n",
    "y_predict = model.predict(x_test)\n",
    "print(y_predict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
